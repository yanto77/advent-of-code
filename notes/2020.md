# Advent Of Code 2020, notes

# Day 00

Purpose of doing these: to have fun and to learn. My expectation is that the solutions can be found relatively easily. Improving on them is what is going to take time and extensive effort. Thus, I'm looking mostly for the journey here, not the destination.

# Day 01

The initial implementation with simplest bruteforce double/triple loop takes 3100 μs [389bd82]. Next natural step would be to ask what and where we could improve it? There is the [PPC course](http://ppc.cs.aalto.fi) at Aalto, with [notes about performance debugging](http://ppc.cs.aalto.fi/2020/debug/perf/), so let's start with those.

Collecting CPU usage statistics can be done with `perf stat`:

```sh
$ perf stat $PROG $ARGS
```

What are we looking for in the statistics?
* CPUs utilized: should be close to 1 for single-threaded, and to N for multi-threaded solutions (where N is the number of available threads). A thread waiting, for e.g., memory operations or other threads, simply wastes time.
* Frontend cycles idle: "The cycles stalled in the front-end are a waste because that means that the Front-End does not feed the Back End with micro-operations. This can mean that you have misses in the Instruction cache, or complex instructions that are not already decoded in the micro-op cache. Just-in-time compiled code usually expresses this behavior." [[ref]](https://stackoverflow.com/a/29059380)
* Backend cycles idle: "The cycles stalled in the back-end are a waste because the CPU has to wait for resources (usually memory) or to finish long latency instructions (e.g. transcedentals - sqrt, reciprocals, divisions, etc.)." [[ref]](https://stackoverflow.com/a/29059380)
* Instructions per cycle: more than 1.0 can be considered good [[1]](http://www.brendangregg.com/perf.html#CPUstatistics), [[2]](http://ppc.cs.aalto.fi/2020/debug/perf/).
* Branch misses: "If a conditional branch usually goes a specific way, the next time it appears, the predictor will assume it will take the same route.". Therefore, sorting input so that there are less randomness with branching outcomes is better. [[ref]](https://www.linux.com/training-tutorials/performance-analysis-linux/)

If there are "not counted" entries, the reason for that may be that the program in question executes for a very short amount of time. The percentage in the parenthesis tells that this counter was counted only for that amount of running time. Splitting the counters into multiple runs seems to help this issue. [[ref]](https://stackoverflow.com/a/37607099)

Collecting events can be done with `perf record && perf report`:

```sh
$ perf record $PROG $ARGS
$ perf report
```

Using default sampling frequency may not be enough. For example, default frequency of 4000 Hz  will result in resolution of 250 μs. Based on [example results from another year](https://github.com/Voltara/advent2018-fast) such resolution would return only 1-2 events for more than half of the solutions. It is possible to increase the frequency to greater values, e.g. 63500 Hz, with `perf record --freq=max`. Such frequency gives resolution of ~15.7 μs, which should be enough.

## Improvements

Sorting input vector is the first thing that can be done, as suggested by [[ref]](https://www.linux.com/training-tutorials/performance-analysis-linux/). The execution time drops to 200 μs. This operation reduces the number of branch misses, which in turn means that the branch predictor is able to keep backend filled with operations. The absolute number of cycles and the ones that stall is reduced by ~70-80%, however, the relative to each other these values are still in range 30-40%.


Some references:
* https://www.agner.org/optimize/optimizing_cpp.pdf



# Day 02

Parsing previous thing was relatively easy. But now we actually have something relatively complex to parse.
- One way would be to implement yourself an algorithm that checks input char by char
    -> ad hoc variant, could also implement checking while reading? we are only counting, so copying the data wouldn't be necessary.
    -> if string_view's are used, then no copies should be made.
- Another option is to use regex?
    -> ctre seems fast. is it fast enough?
- Anything else?

Base solution seems to take approx 600 μs.

# Day 03

Base solution takes approx. 60 μs. There isn't really any point to optimize that.

As an exercise, there is the constant input data which we would need to analyze with different parameters. As is, it sounds like a perfect setup for multithreading, right? Maybe, but multiple threads or processes have their own management overhead, so when would it be reasonable to share this task?

* Adding simple "#pragma omp parallel for" to process each slope separately makes the final time be approx 1300 μs. Sometimes though the runtime jumps up to as high as 7000 μs, and probably even higher in some situations. Therefore, we should consider adding multithreading next time when the runtime is more than 10 ms.

# Day 04

* switch by key found in input. assumption: comparing strings (and string_views) is complex, but if they are hashed, comparison should be faster.

* initial:
    Day 04: valid passes: 237
    [339 μs]
    Total: 339 μs

# Day 05

Interesting technique is to add static_assert checks. Easier to construct the initial line conversion, when there are some example inputs and outputs for a method. Iteration speed is also great, as there's no need to run additional command -- the compile command both compiles and "runs" the code.

> Here are some other boarding passes:
>
>    BFFFBBFRRR: row 70, column 7, seat ID 567.
>    FFFBBBFRRR: row 14, column 7, seat ID 119.
>    BBFFBBFRLL: row 102, column 4, seat ID 820.

Which then converts to:

```cpp
constexpr std::pair<uint8_t, uint8_t> convert_line(const std::string_view& view)
{
    ...
}

static_assert(convert_line("BFFFBBFRRR") == std::pair<uint8_t, uint8_t>{0, 0});
static_assert(convert_line("FFFBBBFRRR") == std::pair<uint8_t, uint8_t>{1, 0});
static_assert(convert_line("BBFFBBFRLL") == std::pair<uint8_t, uint8_t>{0, 1});
```

Otherwise, the input is a key to the binary tree, which just needs to be translated.

Initial solution to both parts takes 21 μs. The major factor here is that the data is static and has size of 1024 entries (128 rows by 8 columns). As such, there isn't really any need to optimize, as the return on invested effort will be minuscule.