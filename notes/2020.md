# Advent Of Code 2020, notes

# Overview

Day 01 = 189 μs
Day 02 = 572 μs
Day 03 =  62 μs
Day 04 = 377 μs
Day 05 =  19 μs
Total:  1220 μs

References:
* https://www.reddit.com/r/adventofcode/wiki/solution_megathreads
* https://github.com/zedrdave/advent_of_code, Python, posts notes/comments
* https://github.com/DarthGandalf/advent-of-code, creates visualizations
* https://github.com/thorstel/Advent-of-Code-2020
* https://github.com/tboschi/advent-of-code-2020
* https://github.com/Bogdanp/awesome-advent-of-code#c-2, link list

See also:
* https://github.com/Voltara/advent2017-fast
* https://github.com/Voltara/advent2018-fast
* https://github.com/Voltara/advent2019-fast
* and probably 2020 version at some point?

# Day 00

Purpose of doing these: to have fun and to learn. My expectation is that the solutions can be found relatively easily. Improving on them is what is going to take time and extensive effort. Thus, I'm looking mostly for the journey here, not the destination.

# Day 01

The initial implementation with simplest bruteforce double/triple loop takes 3100 μs [389bd82]. Next natural step would be to ask what and where we could improve it? There is the [PPC course](http://ppc.cs.aalto.fi) at Aalto, with [notes about performance debugging](http://ppc.cs.aalto.fi/2020/debug/perf/), so let's start with those.

Collecting CPU usage statistics can be done with `perf stat`:

```sh
$ perf stat $PROG $ARGS
```

What are we looking for in the statistics?
* CPUs utilized: should be close to 1 for single-threaded, and to N for multi-threaded solutions (where N is the number of available threads). A thread waiting, for e.g., memory operations or other threads, simply wastes time.
* Frontend cycles idle: "The cycles stalled in the front-end are a waste because that means that the Front-End does not feed the Back End with micro-operations. This can mean that you have misses in the Instruction cache, or complex instructions that are not already decoded in the micro-op cache. Just-in-time compiled code usually expresses this behavior." [[ref]](https://stackoverflow.com/a/29059380)
* Backend cycles idle: "The cycles stalled in the back-end are a waste because the CPU has to wait for resources (usually memory) or to finish long latency instructions (e.g. transcedentals - sqrt, reciprocals, divisions, etc.)." [[ref]](https://stackoverflow.com/a/29059380)
* Instructions per cycle: more than 1.0 can be considered good [[1]](http://www.brendangregg.com/perf.html#CPUstatistics), [[2]](http://ppc.cs.aalto.fi/2020/debug/perf/).
* Branch misses: "If a conditional branch usually goes a specific way, the next time it appears, the predictor will assume it will take the same route.". Therefore, sorting input so that there are less randomness with branching outcomes is better. [[ref]](https://www.linux.com/training-tutorials/performance-analysis-linux/)

If there are "not counted" entries, the reason for that may be that the program in question executes for a very short amount of time. The percentage in the parenthesis tells that this counter was counted only for that amount of running time. Splitting the counters into multiple runs seems to help this issue. [[ref]](https://stackoverflow.com/a/37607099)

Collecting events can be done with `perf record && perf report`:

```sh
$ perf record $PROG $ARGS
$ perf report
```

Using default sampling frequency may not be enough. For example, default frequency of 4000 Hz  will result in resolution of 250 μs. Based on [example results from another year](https://github.com/Voltara/advent2018-fast) such resolution would return only 1-2 events for more than half of the solutions. It is possible to increase the frequency to greater values, e.g. 63500 Hz, with `perf record --freq=max`. Such frequency gives resolution of ~15.7 μs, which should be enough.

## Improvements

Sorting input vector is the first thing that can be done, as suggested by [[ref]](https://www.linux.com/training-tutorials/performance-analysis-linux/). The execution time drops to 200 μs. This operation reduces the number of branch misses, which in turn means that the branch predictor is able to keep backend filled with operations. The absolute number of cycles and the ones that stall is reduced by ~70-80%, however, the relative to each other these values are still in range 30-40%.


Some references:
* https://www.agner.org/optimize/optimizing_cpp.pdf



# Day 02

Parsing previous thing was relatively easy. But now we actually have something relatively complex to parse.
- One way would be to implement yourself an algorithm that checks input char by char
    -> ad hoc variant, could also implement checking while reading? we are only counting, so copying the data wouldn't be necessary.
    -> if string_view's are used, then no copies should be made.
- Another option is to use regex?
    -> ctre seems fast. is it fast enough?
- Anything else?

Base solution seems to take approx 600 μs.

# Day 03

Base solution takes approx. 60 μs. There isn't really any point to optimize that.

As an exercise, there is the constant input data which we would need to analyze with different parameters. As is, it sounds like a perfect setup for multithreading, right? Maybe, but multiple threads or processes have their own management overhead, so when would it be reasonable to share this task?

* Adding simple "#pragma omp parallel for" to process each slope separately makes the final time be approx 1300 μs. Sometimes though the runtime jumps up to as high as 7000 μs, and probably even higher in some situations. Therefore, we should consider adding multithreading next time when the runtime is more than 10 ms.

# Day 04

* switch by key found in input. assumption: comparing strings (and string_views) is complex, but if they are hashed, comparison should be faster.

* initial solution (for part1) takes 350 μs, for both parts (with input validation): 420 μs

* by default, `passport_t` looks as follows. what if instead of using full types, we could use our knowledge of limits of our input data and reduce the size of this class? test it out with uint8/16_t instead of ints.

```cpp
    // State
    uint8_t m_filled = 0;
    uint8_t m_valid = 0;

    // Data
    int m_byr;                  => uint16_t m_byr;
    std::string_view m_cid;
    EyeColor m_ecl;
    int m_eyr;                  => uint16_t m_eyr;
    int m_hcl;
    std::pair<int, bool> m_hgt; => std::pair<uint8_t, bool> m_hgt;
    int m_iyr;                  => uint16_t m_iyr;
    int m_pid;                  => uint32_t m_pid;
```

by doing that:
* 56 bytes -> 40 bytes. In total, there are 290 passports, so total memory size has been reduced by 4640 bytes. Not a big issue with these data sizes.
* `perf stat` output hasn't really changed much.

maybe some gains could be noticed by optimizing away until 32 bytes are met and aligning allocated memory?

* hashing these small strings also didn't help, but instead increased the runtime to 4000-5000 μs.

# Day 05

Interesting technique is to add static_assert checks. Easier to construct the initial line conversion, when there are some example inputs and outputs for a method. Iteration speed is also great, as there's no need to run additional command -- the compile command both compiles and "runs" the code.

> Here are some other boarding passes:
>
>    BFFFBBFRRR: row 70, column 7, seat ID 567.
>    FFFBBBFRRR: row 14, column 7, seat ID 119.
>    BBFFBBFRLL: row 102, column 4, seat ID 820.

Which then converts to:

```cpp
constexpr std::pair<uint8_t, uint8_t> convert_line(const std::string_view& view)
{
    ...
}

static_assert(convert_line("BFFFBBFRRR") == std::pair<uint8_t, uint8_t>{0, 0});
static_assert(convert_line("FFFBBBFRRR") == std::pair<uint8_t, uint8_t>{1, 0});
static_assert(convert_line("BBFFBBFRLL") == std::pair<uint8_t, uint8_t>{0, 1});
```

Otherwise, the input is a key to the binary tree, which just needs to be translated.

Initial solution to both parts takes 21 μs. The major factor here is probably that the data is static and has size of 1024 entries (128 rows by 8 columns). As such, there isn't really any need to optimize, as the return on invested effort will be minuscule.

Later, found out [this solution](https://github.com/tboschi/advent-of-code-2020/blob/master/day05/cpp/src/part2.cpp). Couple of notes:

* It didn't occur to me that we were evaluating one single binary tree, not two... That simplifies things a bit.
    -> Although, most probably it simplifies only the code, not the actual implementation. As the entries are already in contiguous 2-dimensional array, so we can make it 1-dimensional, but the memory layout is still the same.

* After trying that solution in my project, it showed that execution is now approx 150 μs, which is magnitude larger than initial solution. Why is that? Couple of guesses:
    * Is it the additional sorting?
        -> Removing sorting (when saving indice numbers) yields wrong results, but the execution time itself drops to 90 μs.
    * Initial solution worked on an array of booleans, where the index marked the seat id. In their solution, they stored size_t indices directly. Is it the additional memory which affects the cache lines etc?
        * each std::vector has 24 byte overhead.
        *
            ```
            // Each array is initialized with 1024 elements.
            std::vector<uint16_t>   v1;
            std::vector<size_t>     v2;
            std::array<bool, 1024>  v3;

            sizeof(bool)        => 1
            sizeof(uint16_t)    => 2
            sizeof(size_t)      => 8

            sizeof(v3)  => 1024
            sizeof(sizeof(v1) + sizeof(uint16_t) * v1.capacity()) => 2072
            sizeof(sizeof(v2) + sizeof(size_t) * v2.capacity()) => 8216
            ```
        * going back to array<bool> didn't get us close to initial solutions performance. with sorting removing, measurements that resulted in values above 100 μs were noticeable somtimes. with this array change, it seems that it doesn't happen as often, so i would guess that changing this had some effect. but, as noticed with previous days, memory doesn't really seem to be the issue here.

    * Is it the unification of `if (sv[i] == 'B' || sv[i] == 'R')` part into one?
        * That was the main thing! This is understandable, because in the inner loop, we evaluate first part of the if-clause for 7 times and then throw away the result. The same goes for the second part, where it depends on the input. Let's assume that input is uniformly distributed, thus the second part is evaluated and thrown away for 7/2 = 3 times.

        Given 10 chars:
            7 chars
                -> 3.5 succeed on first if, second if not evaluated
                -> 3.5 fail on first if, and evaluate 2nd if unnecessarily
            3 chars
                -> evaluates first if 3 times, all are unnecessary
                -> evaluation of 2nd if is always necessary
            In total: 3.5 + 3 unnecessary if evaluations. We only need 10 though, so 16.5 is too much. Strange that reducing number of possible ifs by ~40% reduces the solution's time from 90 μs to 20-25 μs (~75%). So maybe there's some compiler optimizations also going in the background? Or maybe it's the branch predictor being reset affecting the result so much?

        -> checking the results with `perf stat -r 1000` shows that on average separating if-clauses helps by reducing branch-misses and stalled-cycles-backend metrics. also IPC increases from 1.30 to 1.37. so, definetly the branch predictor like the version with separated if-clauses better.

# Day 07

* Purpose of hashing string?
    * We are comparing longer strings made of two parts. So, comparing just first part would require comparing only couple of characters. But with second part, some comparisons will require much more time, as they will need to evaluate ~10 chars to only reach the part that matters.

* When having a matrix, there will be bunch of empty space.
    * Spend some time to compact? Would help at least initial state.
    * The lines that have only one value -> they are ending point, they only have themselves. So push them in the end.